from langgraph.graph import StateGraph
import os
from dotenv import load_dotenv

from langchain_openai import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field


##########


load_dotenv()

AOAI_API_KEY = os.getenv("AOAI_API_KEY")
AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT")
AOAI_DEPLOY_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O")
AOAI_API_VERSION = "2024-02-15-preview"

llm_gpt4o = AzureChatOpenAI(
    openai_api_key=AOAI_API_KEY,
    azure_endpoint=AOAI_ENDPOINT,
    api_version=AOAI_API_VERSION,
    deployment_name=AOAI_DEPLOY_GPT4O,
    temperature=0.7,
    max_tokens=1000,
    model_kwargs={"response_format": {"type": "json_object"}},
)


##########


class PromptOutput(BaseModel):
    system_prompt: str = Field(
        ..., description="AI ì‘ë‹µì„ ìœ„í•œ ìµœì í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤."
    )
    user_prompt_template: str = Field(
        ..., description="ì‚¬ìš©ì ì…ë ¥ì„ í¬í•¨í•  ìˆ˜ ìˆëŠ” ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì…ë‹ˆë‹¤."
    )
    reasoning: str = Field(
        ...,
        description="ì´ í”„ë¡¬í”„íŠ¸ë“¤ì´ ì‚¬ìš©ì ì˜ë„ë¥¼ ì–´ë–»ê²Œ íš¨ê³¼ì ìœ¼ë¡œ ë°˜ì˜í•˜ëŠ”ì§€ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
    )


prompt_parser = PydanticOutputParser(pydantic_object=PromptOutput)


##########


def run_prompt_generation_agent(user_intention, user_sys_params=None):
    """
    ì‚¬ìš©ì ì˜ë„(user_intention)ì™€ ì„ íƒì  ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì•„
    ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        user_intention (str): ì‚¬ìš©ìì˜ ëª©ì ê³¼ ì›í•˜ëŠ” AI ì‘ë‹µ ìœ í˜•ì— ëŒ€í•œ ì„¤ëª…
        user_sys_params (str, optional): ìƒì„±í•  í”„ë¡¬í”„íŠ¸ ìœ í˜• ("user" ë˜ëŠ” "system")
                                        ê¸°ë³¸ê°’ì€ "system"

    Returns:
        dict: ìµœì¢… ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” ìƒíƒœ ë”•ì…”ë„ˆë¦¬
    """

    if user_sys_params is None or user_sys_params not in ["user", "system"]:
        user_sys_params = "system"

    state = {
        "user_intention": user_intention,
        "prompt_type": user_sys_params,
        "prompt_draft": None,
        "prompt_refined": None,
        "final_prompt": None,
    }

    def analyze_intention(state):
        """ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì´ˆì•ˆì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        user_intention = state["user_intention"]
        prompt_type = state["prompt_type"]

        type_guidance = ""
        if prompt_type == "user":
            type_guidance = "ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ëŠ” AIì—ê²Œ ì§ì ‘ ì „ë‹¬ë˜ëŠ” ì§ˆë¬¸ì´ë‚˜ ì§€ì‹œì‚¬í•­ì…ë‹ˆë‹¤."
        else:  # system
            type_guidance = "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” AIì˜ ì—­í• ê³¼ ë™ì‘ì„ ì •ì˜í•˜ëŠ” ì§€ì¹¨ì…ë‹ˆë‹¤."

        system_message = f"""ë‹¹ì‹ ì€ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì˜ë„ì™€ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ íš¨ê³¼ì ì¸ AI í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„í•´ì£¼ì„¸ìš”.
{type_guidance}
ì‚¬ìš©ì ì˜ë„ë¥¼ ì´í•´í•˜ê³  ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì´ˆì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "system_prompt": "ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì´ˆì•ˆ",
  "user_prompt_template": "ì´ˆê¸° ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿",
  "reasoning": "ì´ í”„ë¡¬í”„íŠ¸ê°€ ì‚¬ìš©ì ì˜ë„ì— ì–´ë–»ê²Œ ë¶€í•©í•˜ëŠ”ì§€ì— ëŒ€í•œ ì´ˆê¸° ë¶„ì„"
}}"""

        prompt_input = (
            f"ë‹¤ìŒ ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì´ˆì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n"
            f"ì‚¬ìš©ì ì˜ë„: {user_intention}\n\n"
            f"í”„ë¡¬í”„íŠ¸ íƒ€ì…: {prompt_type} í”„ë¡¬í”„íŠ¸\n\n"
            "JSON í˜•ì‹ìœ¼ë¡œ ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì œì•ˆí•´ì£¼ì„¸ìš”."
        )

        try:
            response = llm_gpt4o.invoke(
                [
                    SystemMessage(content=system_message),
                    HumanMessage(content=prompt_input),
                ]
            )

            parsed_output = PromptOutput.model_validate_json(response.content)

            state["prompt_draft"] = {
                "system_prompt": parsed_output.system_prompt.strip(),
                "user_prompt_template": parsed_output.user_prompt_template.strip(),
                "reasoning": parsed_output.reasoning.strip(),
            }

        except Exception as e:
            print(f"[ERROR] ì˜ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            state["prompt_draft"] = {
                "system_prompt": "ì˜¤ë¥˜ë¡œ ì¸í•´ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "user_prompt_template": "ì˜¤ë¥˜ë¡œ ì¸í•´ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "reasoning": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            }

        return state

    def refine_prompt(state):
        """ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì´ˆì•ˆì„ ê°œì„ í•©ë‹ˆë‹¤."""
        draft = state["prompt_draft"]
        user_intention = state["user_intention"]
        prompt_type = state["prompt_type"]

        type_specific_guidance = ""
        if prompt_type == "user":
            type_specific_guidance = "ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ë” ì§‘ì¤‘í•˜ì—¬ ê°œì„ í•˜ì„¸ìš”."
        else:
            type_specific_guidance = "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë” ì§‘ì¤‘í•˜ì—¬ ê°œì„ í•˜ì„¸ìš”."

        system_message = f"""ë‹¹ì‹ ì€ ìµœê³  ìˆ˜ì¤€ì˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì´ˆì•ˆì„ ê²€í† í•˜ê³  ë” íš¨ê³¼ì ì´ê³  ì •êµí•œ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.
{type_specific_guidance}
ëª…í™•ì„±, íš¨ê³¼ì„±, ì‚¬ìš©ì ì˜ë„ ë°˜ì˜ ì¸¡ë©´ì—ì„œ ê°œì„ ì ì„ ì°¾ì•„ ì ìš©í•˜ì„¸ìš”.
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "system_prompt": "ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
  "user_prompt_template": "ê°œì„ ëœ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿",
  "reasoning": "ê°œì„  ì‚¬í•­ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…ê³¼ íš¨ê³¼ì ì¸ ì´ìœ "
}}"""

        prompt_input = (
            f"ë‹¤ìŒ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì´ˆì•ˆì„ ê²€í† í•˜ê³  ê°œì„ í•´ì£¼ì„¸ìš”:\n\n"
            f"ì‚¬ìš©ì ì˜ë„: {user_intention}\n\n"
            f"í”„ë¡¬í”„íŠ¸ íƒ€ì…: {prompt_type} í”„ë¡¬í”„íŠ¸\n\n"
            f"ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸:\n\"{draft['system_prompt']}\"\n\n"
            f"ì´ˆê¸° ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿:\n\"{draft['user_prompt_template']}\"\n\n"
            "ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ê°œì„ í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
        )

        try:
            response = llm_gpt4o.invoke(
                [
                    SystemMessage(content=system_message),
                    HumanMessage(content=prompt_input),
                ]
            )

            parsed_output = PromptOutput.model_validate_json(response.content)

            state["prompt_refined"] = {
                "system_prompt": parsed_output.system_prompt.strip(),
                "user_prompt_template": parsed_output.user_prompt_template.strip(),
                "reasoning": parsed_output.reasoning.strip(),
            }

        except Exception as e:
            print(f"[ERROR] í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            state["prompt_refined"] = state["prompt_draft"]

        return state

    def finalize_prompt(state):
        """ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ê²°ì •í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤."""
        refined = state["prompt_refined"]
        user_intention = state["user_intention"]
        prompt_type = state["prompt_type"]

        type_specific_guidance = ""
        if prompt_type == "user":
            type_specific_guidance = "ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ì‚¬ìš©ì ì˜ë„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        else:
            type_specific_guidance = (
                "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ AIì˜ ì—­í• ê³¼ í–‰ë™ì„ ëª…í™•í•˜ê²Œ ì •ì˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
            )

        system_message = f"""ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì¢… ê²€í† í•˜ê³ , ì‚¬ìš©ì ì˜ë„ì— ê°€ì¥ ì í•©í•œ ìµœì¢… ë²„ì „ì„ í™•ì •í•´ì£¼ì„¸ìš”.
{type_specific_guidance}
ìµœì¢… í”„ë¡¬í”„íŠ¸ëŠ” êµ¬ì²´ì ì´ê³ , ëª…í™•í•˜ë©°, ì‚¬ìš©ì ì˜ë„ë¥¼ ì •í™•íˆ ì¶©ì¡±í•´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "system_prompt": "ìµœì¢… í™•ì •ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
  "user_prompt_template": "ìµœì¢… í™•ì •ëœ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿",
  "reasoning": "ìµœì¢… í”„ë¡¬í”„íŠ¸ê°€ ì‚¬ìš©ì ì˜ë„ë¥¼ ì–´ë–»ê²Œ ì¶©ì¡±í•˜ëŠ”ì§€ì™€ ì˜ˆìƒë˜ëŠ” íš¨ê³¼ì— ëŒ€í•œ ìµœì¢… í‰ê°€"
}}"""

        prompt_input = (
            f"ë‹¤ìŒ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì¢… ê²€í† í•˜ê³  í™•ì •í•´ì£¼ì„¸ìš”:\n\n"
            f"ì‚¬ìš©ì ì˜ë„: {user_intention}\n\n"
            f"í”„ë¡¬í”„íŠ¸ íƒ€ì…: {prompt_type} í”„ë¡¬í”„íŠ¸\n\n"
            f"ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸:\n\"{refined['system_prompt']}\"\n\n"
            f"ê°œì„ ëœ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿:\n\"{refined['user_prompt_template']}\"\n\n"
            "ìµœì¢… í™•ì •ëœ í”„ë¡¬í”„íŠ¸ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ê³  ê·¸ íš¨ê³¼ì— ëŒ€í•œ ìµœì¢… í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”."
        )

        try:
            response = llm_gpt4o.invoke(
                [
                    SystemMessage(content=system_message),
                    HumanMessage(content=prompt_input),
                ]
            )

            parsed_output = PromptOutput.model_validate_json(response.content)

            state["final_prompt"] = {
                "system_prompt": parsed_output.system_prompt.strip(),
                "user_prompt_template": parsed_output.user_prompt_template.strip(),
                "reasoning": parsed_output.reasoning.strip(),
            }

        except Exception as e:
            print(f"[ERROR] í”„ë¡¬í”„íŠ¸ í™•ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            state["final_prompt"] = state["prompt_refined"]

        return state

    def generate_report(state):
        """í”„ë¡¬í”„íŠ¸ ìƒì„± ê³¼ì •ê³¼ ìµœì¢… ê²°ê³¼ì— ëŒ€í•œ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        final = state["final_prompt"]
        prompt_type = state["prompt_type"]

        print("--í”„ë¡¬í”„íŠ¸ ìƒì„± ìµœì¢… ê²°ê³¼--\n")

        if prompt_type == "user":
            print(
                f"ğŸ”¹ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì£¼ìš” ê²°ê³¼ë¬¼):\n{final['user_prompt_template']}\n"
            )
            print(f"ğŸ”¹ ì¶”ì²œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸:\n{final['system_prompt']}\n")
        else:
            print(f"ğŸ”¹ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì£¼ìš” ê²°ê³¼ë¬¼):\n{final['system_prompt']}\n")
            print(f"ğŸ”¹ ì¶”ì²œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿:\n{final['user_prompt_template']}\n")

        print(f"ğŸ”¹ í”„ë¡¬í”„íŠ¸ ì„¤ê³„ ê·¼ê±°:\n{final['reasoning']}\n")

        return state

    ##########

    graph = StateGraph(dict)

    graph.add_node("Analyze Intention", analyze_intention)
    graph.add_node("Refine Prompt", refine_prompt)
    graph.add_node("Finalize Prompt", finalize_prompt)
    graph.add_node("Generate Report", generate_report)

    graph.set_entry_point("Analyze Intention")
    graph.add_edge("Analyze Intention", "Refine Prompt")
    graph.add_edge("Refine Prompt", "Finalize Prompt")
    graph.add_edge("Finalize Prompt", "Generate Report")

    graph_executor = graph.compile()

    result = graph_executor.invoke(state)
    return result


def generate_prompt_by_intention(user_intention, prompt_type):
    """
    ì‚¬ìš©ì ì˜ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        user_intention (str): ì‚¬ìš©ìê°€ ì›í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì˜ ì˜ë„/ëª©ì 
        prompt_type (str): 'user' ë˜ëŠ” 'system' í”„ë¡¬í”„íŠ¸ ìœ í˜•

    Returns:
        dict: ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    result = run_prompt_generation_agent(user_intention, prompt_type)

    if result and result.get("final_prompt"):
        final_prompt = result["final_prompt"]

        if prompt_type == "user":
            return {
                "prompt": final_prompt["user_prompt_template"],
                "reasoning": final_prompt["reasoning"],
            }
        else:  # system
            return {
                "prompt": final_prompt["system_prompt"],
                "reasoning": final_prompt["reasoning"],
            }

    return {
        "prompt": f"ìƒì„± ì‹¤íŒ¨: {prompt_type} í”„ë¡¬í”„íŠ¸ ({user_intention})",
        "reasoning": "í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
    }
